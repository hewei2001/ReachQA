<div align=center><h1>
    ðŸª„Distill Visual Chart Reasoning Ability<br>
    from LLMs to MLLMs
</h1></div>

This is the official repository for ðŸ“ƒ [Distill Visual Chart Reasoning Ability from LLMs to MLLMs](#).

You have two options to obtain our dataset:

1. Download directly from the ðŸ¤—**HuggingFace** Dataset: [ReachQA](#)
2. Clone this repository and **generate ðŸ“ˆcharts using the synthetic code**: The process takes about **3 minutes**!

## ðŸ“–Introduction

### Code-as-Intermediary Translation

We propose **Code-as-Intermediary Translation (CIT)**, a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities **from LLMs to MLLMs**.  The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce **ReachQA**, a dataset containing 3k **rea**soning-intensive **ch**arts and 20k Q&A pairs to enhance both recognition and reasoning abilities.  Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks such as MathVista.

<div align=center><img src="./assets/ReachQA.jpg" width="90%" /></div>

> Figure: Overview of the CIT method for synthesizing multimodal instruction data. The process begins with **33 seed codes** and generates plot codes across various chart types, topics, and complexity levels through the Self-Instruct and Evol-Instruct stages. The chart set and instruction set are constructed bi-directionally, and the final filtered data yields ReachQA, a dataset for distilling visual chart reasoning abilities from LLMs to MLLMs.

### ReachQA

> Table: Comparison of existing chart-related datasets across **three properties**. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat.=rationale, Annot.=annotation and Scal.=scalable.

<div align=center><img src="./assets/Compare.png" width="90%" /></div>

> Table: ReachQA dataset statistics. Question and answer lengths are calculated based on the GPT-4o tokenizer.

| **Statistics**         | **Train Set** | **Test Set** |
| ---------------------- | ------------- | ------------ |
| **Total charts**       | 3,249         | 500          |
| - # Chart types        | 10 / 32       | 10 / 32      |
| - # Overlay plots      | 1,030         | 220          |
| - # Multiple plots     | 593           | 251          |
| - Average size (px)    | 2480Ã—1571     | 2798Ã—1601    |
| **Unique questions**   | 19,963        | 2,000        |
| - # Reco. per chart    | 2.53          | 2            |
| - # Reas. per chart    | 3.62          | 2            |
| **Average Length**     |               |              |
| - Avg. Reco. Q. length | 22.1          | 21.0         |
| - Avg. Reco. A. length | 38.3          | 7.0          |
| - Avg. Reas. Q. length | 38.2          | 35.4         |
| - Avg. Reas. A. length | 68.4          | 24.9         |

## âš¡ï¸Install

1. For dataset usage:
```bash
git clone https://github.com/hewei2001/ReachQA.git
cd ReachQA
conda create -n ReachQA_data python=3.10 -y
conda activate ReachQA_data

pip install -r requirements_data.txt
pip install lmdeploy # Optional, for MLLM filter
```

2. For training / evaluation usage:
```Shell
git clone https://github.com/hewei2001/ReachQA.git
cd ReachQA
conda create -n ReachQA_train python=3.10 -y
conda activate ReachQA_train

git clone https://github.com/modelscope/swift.git
cd swift
pip install -e '.[llm]'
pip install timm
```

## ðŸŒ³Project Structure

```
ReachQA
 â”œâ”€â”€ assets
 â”œâ”€â”€ data
 â”‚   â”œâ”€â”€ reachqa_seed
 â”‚   â”œâ”€â”€ reachqa_test
 â”‚   â””â”€â”€ reachqa_train
 â”œâ”€â”€ scripts
 â”‚   â”œâ”€â”€ data
 â”‚   â”œâ”€â”€ eval
 â”‚   â”œâ”€â”€ filter
 â”‚   â””â”€â”€ train
 â”œâ”€â”€ utils
 â”‚   â”œâ”€â”€ chart_notes.py
 â”‚   â”œâ”€â”€ openai_utils.py
 â”‚   â””â”€â”€ __init__.py
 â”œâ”€â”€ batch_filter_image.py
 â”œâ”€â”€ batch_filter_QA.py
 â”œâ”€â”€ openai_generate_code.py
 â”œâ”€â”€ openai_generate_QA.py
 â”œâ”€â”€ openai_llm_evaluation.py
 â”œâ”€â”€ swift_infer_dataset.py
 â”œâ”€â”€ requirements_data.txt
 â””â”€â”€ README.md
```
| File                     | Description                                |
|--------------------------|--------------------------------------------|
| assets/                  | Folder for project-related resources       |
| data/                    | Folder for dataset storage                 |
| scripts/                 | Folder for scripts to run |
| utils/                   | Folder for utility functions               |
| batch_filter_QA.py      | Code for filtering Q&A with MLLMs |
| batch_filter_image.py    | Code for filtering images with MLLMs |
| openai_generate_QA.py    | Code for synthesizing Q&A |
| openai_generate_code.py  | Code for synthesizing code for charts |
| openai_llm_evaluation.py | Code for LLM-as-a-Jugde evaluation |

## ðŸ“ˆQuick Start

1. **Obtain ReachQA dataset in 3 minutes:**

```bash
cd ReachQA
conda activate ReachQA_data

python ./data/reachqa_train/execute_code.py \
	--code_dir ./data/reachqa_train/code/ \
	--image_dir ./data/reachqa_train/images/ 
	
python ./data/reachqa_test/execute_code.py 
	--code_dir ./data/reachqa_test/code/ \
	--image_dir ./data/reachqa_test/images/ 
```

2. **Data Construction with CIT:**

Before generating, the parameters in the `scripts/` should be modified!

```bash
cd ReachQA
conda activate ReachQA_data

# Generate code
bash ./scripts/data/run_openai_generate_code.sh

# Execute code and generate images
python ./data/reachqa_train/execute_code.py \
	--code_dir ./data/reachqa_train/all_code/ \
	--image_dir ./data/reachqa_train/all_images/ 

# Filter images
bash ./scripts/filter/run_rating_images.sh
python ./data/reachqa_train/filter_rated_image.py \
	--data_dir ./data/reachqa_train/

# Generate QA
bash ./scripts/data/run_openai_generate_QA.sh

# Filter QA
bash ./scripts/filter/run_rating_QA.sh
python ./data/reachqa_train/filter_rated_QA.py \
	--data_dir ./data/reachqa_train/
```

3. **Training / Inference / Evaluation:**

Before training, the JSON instruction file needs to be processed into **Swift format**!

For the specific format, refer to the [Official Swift Documentation](https://github.com/modelscope/ms-swift/tree/main).
```bash
cd ReachQA
conda activate ReachQA_train

# Swift format
cd ./data/reachqa_train/
python process_to_swift_internvl.py

# Training
cd ../..
bash ./scripts/train/internvl2_lora.sh

# Inference
bash ./scripts/eval/infer_InternVL2-8B.sh

# Evaluation
bash ./scripts/eval/run_openai_evaluation.sh
```
## ðŸ“ˆMain Results

> Table: Evaluation results on seven benchmarks. Details for these benchmarks and models are presented in Â§ 4.1. The best performance for each category and task is in **bold**. The percentage of performance improvements compared to the vanilla model is denoted by (â†‘).

<div align=center><img src="./assets/results.png" width="90%" /></div>

---

<div align=center><img src="./assets/attention.png" width="90%" /></div>

> FIgure: An example of **attention visualization** from the ChartQA dataset. The top row shows the results from the vanilla LLaVA-Next-Llama3-8B model, while the bottom row displays the results from our fine-tuned model. For each output, we present the attention distribution (highlighted zones) at **three key steps**, calculated by averaging the attention values of all tokens in each step.


## ðŸ”ŽCitation

```
TODO
```
